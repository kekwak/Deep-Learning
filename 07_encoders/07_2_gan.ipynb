{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a564d5f",
   "metadata": {
    "id": "3a564d5f"
   },
   "source": [
    "#  GAN\n",
    "\n",
    "__Автор задач: Блохин Н.В. (NVBlokhin@fa.ru)__\n",
    "\n",
    "Материалы:\n",
    "* https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
    "* https://www.kaggle.com/datasets/splcher/animefacedataset\n",
    "* https://github.com/eriklindernoren/PyTorch-GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ecd663",
   "metadata": {
    "id": "c9ecd663"
   },
   "source": [
    "## Задачи для совместного разбора"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72ddd13",
   "metadata": {
    "id": "f72ddd13"
   },
   "source": [
    "1\\. Обсудите основные шаги в обучении GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ssEX5gVJJKMu",
   "metadata": {
    "executionInfo": {
     "elapsed": 250,
     "status": "ok",
     "timestamp": 1732870842831,
     "user": {
      "displayName": "Никита Блохин",
      "userId": "16402972581398673009"
     },
     "user_tz": -180
    },
    "id": "ssEX5gVJJKMu"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "gen = nn.Sequential(\n",
    "    nn.ConvTranspose2d(100, 8, 4, 2, 1),\n",
    "    nn.Tanh()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2w4a1trsJrDz",
   "metadata": {
    "executionInfo": {
     "elapsed": 251,
     "status": "ok",
     "timestamp": 1732870869309,
     "user": {
      "displayName": "Никита Блохин",
      "userId": "16402972581398673009"
     },
     "user_tz": -180
    },
    "id": "2w4a1trsJrDz"
   },
   "outputs": [],
   "source": [
    "noise = torch.randn(16, 100, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-wXR5M5JJw-N",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 251,
     "status": "ok",
     "timestamp": 1732870880630,
     "user": {
      "displayName": "Никита Блохин",
      "userId": "16402972581398673009"
     },
     "user_tz": -180
    },
    "id": "-wXR5M5JJw-N",
    "outputId": "3e164105-45ae-4d0c-d2ff-3be5fa4d2e80"
   },
   "outputs": [],
   "source": [
    "generated = gen(noise)\n",
    "generated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "AyeVdFQUJy1V",
   "metadata": {
    "executionInfo": {
     "elapsed": 251,
     "status": "ok",
     "timestamp": 1732870903081,
     "user": {
      "displayName": "Никита Блохин",
      "userId": "16402972581398673009"
     },
     "user_tz": -180
    },
    "id": "AyeVdFQUJy1V"
   },
   "outputs": [],
   "source": [
    "faked_labels = torch.zeros(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "tFuHe76QJ4bO",
   "metadata": {
    "executionInfo": {
     "elapsed": 216,
     "status": "ok",
     "timestamp": 1732870945392,
     "user": {
      "displayName": "Никита Блохин",
      "userId": "16402972581398673009"
     },
     "user_tz": -180
    },
    "id": "tFuHe76QJ4bO"
   },
   "outputs": [],
   "source": [
    "real = torch.randn(16, 8, 2, 2)\n",
    "real_labels = torch.ones(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0KwzuzOXJ-vI",
   "metadata": {
    "executionInfo": {
     "elapsed": 268,
     "status": "ok",
     "timestamp": 1732871019490,
     "user": {
      "displayName": "Никита Блохин",
      "userId": "16402972581398673009"
     },
     "user_tz": -180
    },
    "id": "0KwzuzOXJ-vI"
   },
   "outputs": [],
   "source": [
    "discriminator = nn.Sequential(\n",
    "    #...\n",
    "    nn.Flatten(start_dim=1),\n",
    "    nn.LazyLinear(out_features=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K7xhDzbMKPbs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1732871020410,
     "user": {
      "displayName": "Никита Блохин",
      "userId": "16402972581398673009"
     },
     "user_tz": -180
    },
    "id": "K7xhDzbMKPbs",
    "outputId": "171d316a-12ee-46f5-a4b7-c5ee91c70ed2"
   },
   "outputs": [],
   "source": [
    "discriminator(generated).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ioHCbqF-KZNR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1732871036634,
     "user": {
      "displayName": "Никита Блохин",
      "userId": "16402972581398673009"
     },
     "user_tz": -180
    },
    "id": "ioHCbqF-KZNR",
    "outputId": "cd567ad1-4cce-4a2a-95d9-503f090a006f"
   },
   "outputs": [],
   "source": [
    "discriminator(real).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7b6d63",
   "metadata": {
    "id": "4d7b6d63"
   },
   "source": [
    "## Задачи для самостоятельного решения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f49190d",
   "metadata": {
    "id": "7f49190d"
   },
   "source": [
    "<p class='task' id='1'></p>\n",
    "\n",
    "1\\. Создайте набор данных на основе датасета `animefacedataset`. Используя преобразования `torchvision`, приведите изображения к одному размеру и нормализуйте их. Выведите на экран несколько примеров изображений.\n",
    "\n",
    "- [ ] Проверено на семинаре"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17826516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, Normalize, ToTensor\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch as th\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1839ee8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = kagglehub.dataset_download('splcher/animefacedataset')\n",
    "print('Path to dataset files:', path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6ec74e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 64\n",
    "batch_size = 128\n",
    "stats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n",
    "\n",
    "transform = Compose([\n",
    "    Resize((64, 64)),\n",
    "    CenterCrop((64, 64)),\n",
    "    ToTensor(),\n",
    "    Normalize(*stats)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81db375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageFolder(path, transform=transform)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84185229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denorm(img_tensors):\n",
    "    return img_tensors * stats[1][0] + stats[0][0]\n",
    "\n",
    "def show_images(images, nmax=64):\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    ax.imshow(make_grid(denorm(images.detach()[:nmax]), nrow=8).permute(1, 2, 0))\n",
    "\n",
    "def show_batch(dl, nmax=64):\n",
    "    images, _ = next(iter(dl))\n",
    "    show_images(images, nmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ca5732",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_batch(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647f00fc",
   "metadata": {
    "id": "647f00fc"
   },
   "source": [
    "<p class='task' id='2'></p>\n",
    "\n",
    "2\\. Реализуйте архитектуру `DCGAN` и обучите модель. Подберите гиперпараметры таким образом, чтобы получаемые изображения стали достаточного качественными (четкими и без существенных дефектов). Во время обучения сохраняйте примеры генерации изображений из случайного шума и сравните, как менялось качество получаемых изображений в процессе обучения.\n",
    "\n",
    "- [ ] Проверено на семинаре"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "228454d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim, nn\n",
    "from torchvision.utils import save_image\n",
    "from IPython.display import display, Image as IPImage\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "import imageio\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a00c65a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = th.device('cuda' if th.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62fc4d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc = 3\n",
    "nz = 128\n",
    "ngf = 64\n",
    "ndf = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4518caa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44bd07de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            \n",
    "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1d781c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator().to(device)\n",
    "generator.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c3357db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde695a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator().to(device)\n",
    "discriminator.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d39e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "fake_images = generator(xb)\n",
    "show_images(fake_images.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aaad6403",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dir = 'generated_images'\n",
    "os.makedirs(sample_dir, exist_ok=True)\n",
    "\n",
    "def save_samples(index, latent_tensors):\n",
    "    fake_images = generator(latent_tensors)\n",
    "    fake_fname = 'generated-images-{0:0=4d}.png'.format(index)\n",
    "    \n",
    "    save_image(denorm(fake_images), os.path.join(sample_dir, fake_fname), nrow=8)\n",
    "    print('Saving', fake_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2a16fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx = 1\n",
    "epochs = 250\n",
    "lr = 2e-4\n",
    "\n",
    "fixed_latent = torch.randn(64, nz, 1, 1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a706ba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_g, losses_d = [], []\n",
    "real_scores, fake_scores = [], []\n",
    "\n",
    "opt_d = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "opt_g = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af610e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for real_images, _ in tqdm(loader, desc=f'Epoch {epoch+1}/{epochs}'):\n",
    "        \n",
    "        discriminator.zero_grad()\n",
    "        real_cpu = real_images.to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        label = torch.full((b_size,), 1, dtype=torch.float, device=device)\n",
    "        output = discriminator(real_cpu).view(-1)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        fake = generator(noise)\n",
    "        label.fill_(0)\n",
    "        output = discriminator(fake.detach()).view(-1)\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        errD = errD_real + errD_fake\n",
    "        opt_d.step()\n",
    "\n",
    "        generator.zero_grad()\n",
    "        label.fill_(1)\n",
    "        output = discriminator(fake).view(-1)\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        opt_g.step()\n",
    "    \n",
    "    print('Epoch [{}/{}], loss_g: {:.4f}, loss_d: {:.4f}'.format(epoch+1, epochs, errG.item(), errD.item()))\n",
    "\n",
    "    save_samples(epoch + start_idx, fixed_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4f7b570",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(generator.state_dict(), 'generator_model.bin')\n",
    "torch.save(discriminator.state_dict(), 'discriminator_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00b0eee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_frames_as_video(filename, images_path):\n",
    "    files = [os.path.join(sample_dir, f) for f in os.listdir(sample_dir) if images_path in f]\n",
    "    files.sort()\n",
    "    \n",
    "    images = []\n",
    "    for fname in files:\n",
    "        img = Image.open(fname)\n",
    "        images.append(img)\n",
    "    \n",
    "    imageio.mimsave(filename, images, duration=0.25, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0358a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_frames_as_video('output.gif', 'generated')\n",
    "display(IPImage(filename='./output.gif'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e77b6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "fake_images = generator(latent).cpu()\n",
    "show_images(fake_images, nmax=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41777dc",
   "metadata": {
    "id": "b41777dc"
   },
   "source": [
    "<p class='task' id='3'></p>\n",
    "\n",
    "3\\. Создайте наборы данных на основе архива `summer2winter_yosemite.zip`. Используя преобразования `torchvision`, приведите изображения к одному размеру и нормализуйте их. Выведите на экран несколько примеров изображений, расположив изображения из одной пары рядом по горизонтали.\n",
    "\n",
    "- [ ] Проверено на семинаре"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2335d311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60b5f3ea",
   "metadata": {
    "id": "60b5f3ea"
   },
   "source": [
    "<p class='task' id='4'></p>\n",
    "\n",
    "4\\. Реализуйте архитектуру `CycleGAN` и обучите модель. Подберите гиперпараметры таким образом, чтобы получаемые изображения стали достаточного качественными (четкими и без существенных дефектов). Во время обучения сохраняйте примеры преобразования (в обе стороны) и  сравните, как менялось качество получаемых изображений в процессе обучения.\n",
    "\n",
    "- [ ] Проверено на семинаре"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36679bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
